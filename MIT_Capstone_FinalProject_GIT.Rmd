---
title: "R Notebook"
output: md_document
---

```{r setup, include=FALSE}
chooseCRANmirror(graphics=FALSE, ind=1)
knitr::opts_chunk$set(echo = TRUE)

```

```{r, echo=FALSE}

knitr::opts_chunk$set(error = TRUE)
```


# MITA Capstone Project - Air Bnb and zillow data challenge
## Submitted by - Dhiraj Tripathi
## Ruid - 182002411
## Under supervision of Professor Sergei Schreider

The data challenge is about a real estate agency willing to invest in New York city. The agency has already learnt that investing in 2 Bedroom properties within New York is profitable. This allows us to narrow our search and focus on other important factors. Our main agenda is to find out the zipcodes which are the most profitable in terms of investment.

We are given two datasets for this challenge:

1) Airbnb :- This dataset contains information about property listings along with other variables like size rank, location, cleaning fee, rental price on daily, weekly and monthly basis, reviews and ratings, zipcodes etc.

2) Zillow data: This dataset contains full price information to own a property in many different zipcodes. It contains the historical price data of the properties by zipcode from April 1996 to June 2017. Each column represents the price of the properties lised in different zipcode for a particular month.

To achieve this, we have to clean the data first, analyze it and visualize it later to get some insights on the zipcodes.

Lets get started with this data challenge by by loading and installing all of the required packages as below:


```{r}
if(!require("plyr")){ # for data analysis
  
  install.packages("tidyverse",dependencies = T)
  
  library(plyr)
  
}

detach(package:plyr) #remove it because it yields issues with tidyverse; however, still need to make sure it's installed for rbind.fill

if(!require("colorspace")){ # is required in system to install another dependent package tidyverse
  
  install.packages("colorspace",dependencies = T)
  
  library(colorspace)
  
}

if(!require("tidyverse")){
  # for data manipulation and aggregation
  install.packages("tidyverse",dependencies = T)
  
  library(tidyverse)
  
}

if(!require("plotly")){
  # for interactive graphics
  install.packages("plotly",dependencies = T)
  
  library(plotly)
  
}


if(!require("forecast")){
  # for time series analysis
  install.packages("forecast",dependencies = T)
  
  library(forecast)
}

if(!require("astsa")){
  # for time series analysis
  install.packages("astsa",dependencies = T)
  
  library(astsa)
}

if(!require("data.table")){
  # to read large data
  install.packages("data.table",dependencies = T)
  
  library(data.table)
  
}

if(!require("Amelia")){
  # for imputing missing values
  install.packages("Amelia",dependencies = T)
  
  library(Amelia)
  
}

if(!require("mice")){
  # for imputing missing values
  install.packages("mice",dependencies = T)
  
  library(mice)
  
}
if(!require("dplyr")){
  # data aggregation
  install.packages("dplyr",dependencies = T)
  
  library(dplyr)
  
}

install.packages("webshot")
webshot::install_phantomjs()
getwd()
```

Before we delve further into analysis, lets list out the assumptions given by CAP1 and also the assumptions that I made.

# Assumptions:

1) The investor will pay for the property in cash.  
2) $1 today is worth the same 100 years from now.  
3) All properties and all square feet within each locale can be assumed to be homogeneous.  
4) Occupancy rate is 75%  
5) We assume that a property gets rented 40% of the times on a daily basis, 40% of the times on a weekly basis and 20% of the times on a monthly basis.  
6) Size Rank is not given in Zillow data hence not considered as a factor in revenue estimation.  

## Reading the CSV files


```{r}
listData<-read.csv("F:/CAP1/listings.csv")
zillData <- read.csv("F:/CAP1/airbnb-zillow-data-challenge-master/Zip_Zhvi_2bedroom.csv")
```

### Check the dimensions of the data:
```{r}
dim (listData)
dim (zillData)
```

As we see , there are too many columns in both the data sets. We will only select the ones which are relevant to us. 

We will first work on the Zillow data and extract the columns. Meaning, we will only select the price data for the 3 years and based on that we will forecast the price of the property in 2018.

```{r}
funct1<-function(temporarydf){
  
  {
  
  temporarydf<-temporarydf[,c(2,3,7,226:262)]

  temporarydf<-subset(temporarydf,City=="New York")

 }

  return(temporarydf)
}

ABC<-funct1(zillData) ## Call the function on Zillow Data
head(ABC,5)

```

Now, we will use the ARIMA model on this extracted dataframe and predict the price data 12 steps ahead and store it in a new dataframe. This new dataframe will be used further in merging the datasets.
```{r}
for(i in 1:nrow(ABC)){
  
  {

Q1<-ABC[,c(4:40)]

W1<-ts(Q1[,c(1:37)],frequency = 12)

## We used the auto.arima function in a different script to find out them best arima model and hence the order (1,0,0)  in the arima model.

ARIMA1<-arima((W1[,i]),order = c(1,0,0), seasonal = list(order=c(1,0,0),period=NA),method = "ML")

pred = predict(ARIMA1,n.ahead = 12)
predictvalue<-pred$pred

ABC$EstimatedPrice[i] = predictvalue[length(predictvalue)]
  }
  
  newABC<-subset(ABC[,c(1,2,3,41)])
  colnames(newABC)[colnames(newABC)=="RegionName"]<-"zipcode"
}

head(newABC)


```

Now that we have the new data frame with forecasted values extracted from the Zillow data, we will move with extracting the Airbnb data so that we can merge both of them and analyze further.

```{r}

filterList <- function(tempdf){
  # Select only relevant columns
  relevantcol <- c("id","zipcode","bedrooms","square_feet","price","weekly_price","monthly_price","cleaning_fee","number_of_reviews","review_scores_rating")
  tempdf <- tempdf[,relevantcol]
  # filter data containing 2 bedrooms
  tempdf <- subset(tempdf,tempdf$bedrooms=="2")
  return(tempdf)
}

filteredListData <- filterList(listData) # call the function

str(filteredListData) # observe the structure of this data



```
```{r}
head(filteredListData)
```

# Merge the Data

```{r}
finalData<-merge(filteredListData,newABC,by=c("zipcode"))
str(finalData) ## see the structure again

summary(finalData) ##print the summary to get the insights.


```
We can notice that there are a lot of NA values in the finalData. So the next step is to clean the data.

# Data Cleaning

```{r}
colnames(finalData) <-  c("zipcode","id","bedrooms","square_feet","price","weekly_price","monthly_price","cleaning_fee","number_of_reviews","review_scores_rating","city","size_rank","current_price")  ## change col names
finalData$city <- factor(finalData$city, levels=c("New York")) ## set a filter where city is specifically New York
```

Select the columns with price information to eliminated the $ symbols.

```{r}
cols <- c("price", "weekly_price", "monthly_price","cleaning_fee")

replace_dollar<-function(x){
  price<-as.numeric(gsub("[$,]","",x))
  return(price)
}

finalData[cols] <- lapply(finalData[cols], replace_dollar) ## call the function to replace the symbols with white spaces

str(finalData) ## see the structure

```

As we observe the structure of this data, we will realize that certain columns like "number of reviews" need to be scaled as the numbers in this column can affect the calculations.
```{r}
normalize <- function(x){
  return((x-min(x))/(max(x)-min(x))) # function to scale variables between 0 and 1
}
finalData["number_of_reviews"] <- lapply(finalData["number_of_reviews"], normalize) 

summary(finalData$number_of_reviews)

```
This shows that scaling has been applied successfully.So we should move forward with imputing the missing data.

```{r}
missingValues <- as.data.frame(colSums(sapply(finalData,is.na)))
library(data.table)
missingValues <- as.data.frame(setDT(missingValues, keep.rownames = TRUE)) ## set a data table with the rownames included
colnames(missingValues)<-c("columnName","totalNA_values")##set the col names

library(dplyr)

install.packages("VIM",repos = "http://cran.us.r-project.org")
library(VIM) ## lets you visualize the missing data. This is part of the package MICE

mice_plot <- aggr(finalData, col=c('navyblue','yellow'),
                  numbers=TRUE, sortVars=TRUE,
                  cex.axis=.7,
                  gap=3)

```


We now move forward with imputing the NA values.

```{r}
missingValues<-missingValues%>%
  ## mutate function lets you modify an existing column and we can also write a function to replace the NA values
  mutate_at(vars(totalNA_values),funs(percentNA_values=.*100/nrow(finalData)))%>%
  arrange(desc(percentNA_values))

head(missingValues,13)


```

```{r}
library(mice)

dataSet<-subset(finalData, select = -c(id,city)) ##removing id and city columns so that we can impute the numeric values in the columns which have missing data

imputeddataSet <- mice(dataSet, m=5, method='cart', printFlag=FALSE) ## use the CART method from MICE package to impute the missing values. See below for the number of logged events.

```

```{r}
complete_dataSet<-complete(imputeddataSet) ## complete the imputed dataset
finaldf_subset<-subset(finalData,select = c(id,city)) ##include the id and city

finaldf_complete <- cbind(complete_dataSet,finaldf_subset) # combining the imputed dataset to add id and city

sum(sapply(finaldf_complete, function(x) { sum(is.na(x)) })) # Check if there are any more NA values 

summary(finaldf_complete) # Check the summary of the dataframe again

```

# Aggregating the data

```{r}
library(dplyr)

# mean of current price and other price attributes

avg_df<-finaldf_complete%>%
        group_by(zipcode)%>%
        summarise_at(vars(square_feet:current_price),mean)

# count number of properties in each zipcode as unique_id

unique_id_df<-finaldf_complete%>% select(zipcode,id)%>%
          group_by(zipcode)%>%
          mutate(vars(id=n_distinct(id)))%>%
          select(zipcode,id)%>%
          distinct()
summary_df<-inner_join(avg_df,unique_id_df,by="zipcode")


summary(summary_df)

```
 As we observe the summary_df, we can notice that most of the zipcodes in New York have more than 50 properties listed. Daily price ranges from $65 to $367.3 

# Exploratory Data Analysis

We now define our new variables in order to construct a model which estimates the revenue based on Airbnb data.

```{r}
## Based on the assumptions as mentioned in the beginning

p_daily=.40 
p_weekly=.40
p_monthly=.20
occupancy_rate<-.75
Quarter_1days<-90
Quarter_2days<-180
Year_days<-365

summary_df$TotalCost<-summary_df$current_price*1.2 ## inflate the price to get the max cost possible to be on the safer side while calculating

summary_df$Review_effect <- normalize(summary_df$review_scores_rating) # scale the review_scores_rating

summary_df$Review_effect<-ifelse(summary_df$Review_effect>0,summary_df$Review_effect,mean(summary_df$Review_effect)) ## if the review effect is scaled to 0 , then substitute it with the mean 

summary_df$Revenue_by_q1<-occupancy_rate*Quarter_1days*((p_daily*summary_df$price)+(p_weekly*summary_df$weekly_price/7)+(p_monthly*summary_df$monthly_price/30))*summary_df$Review_effect #calculate the fisrt quarter revenue

summary_df$Revenue_by_q2<-occupancy_rate*Quarter_2days*((p_daily*summary_df$price)+(p_weekly*summary_df$weekly_price/7)+(p_monthly*summary_df$monthly_price/30))*summary_df$Review_effect #calculate the second quarter revenue

summary_df$Revenue_by_year<-occupancy_rate*Year_days*((p_daily*summary_df$price)+(p_weekly*summary_df$weekly_price/7)+(p_monthly*summary_df$monthly_price/30))*summary_df$Review_effect #calculate the yearly revenue

# Obtain the Revenue by amount spend ratio for first quarter

summary_df$Revenue_by_Cost_RatioQ1<-normalize(summary_df$Revenue_by_q1/summary_df$TotalCost)    

# percentage of properties listed for the given zipcode 

summary_df$Percent_units <- normalize(summary_df$id*100/sum(summary_df$id))

summary_df$Cost_by_Revenue <- summary_df$TotalCost/summary_df$Revenue_by_year



```

We have constructed the model to determine revenue and other paramters. We will thus move on the Visualization part.

# Visualizing the data

```{r}
plot_my_graph <- function(col_name){
  # sort the dataframe for by col_name in descending order and subset for top N zipcodes
  
  v <- enquo(col_name)
  
  n=10 # select top n values
  
  df_sorted_unique_id <- arrange(summary_df[summary_df$id>10,],desc(!!v)) [1:n,] # order data
  
  # reassign factor levels
  
  df_sorted_unique_id$zipcode <- factor(df_sorted_unique_id$zipcode)
  
  
  # Return zipcodes
  
  return(df_sorted_unique_id)
  
}


```

 We write a function to order any relevant column and return zipcodes and sort the listings. This ordering function will be used to visualize the data further.
 
 
```{r}
library(plotly)
```

# Visualize the trend between Revenue and Total Cost 

```{r Code Chunk, fig.width=10, fig.height=10}
plot_ly(summary_df, y = ~Revenue_by_q1, x = ~TotalCost, text = ~zipcode, type = 'scatter', mode = 'markers', size = 2000,
        color = ~Revenue_by_Cost_RatioQ1, 
        marker = list(opacity = 0.4, sizemode = 'diameter')) %>%
  layout(title = 'Revenue vs Total Cost',
         xaxis = list(showgrid = T),
         yaxis = list(showgrid = T),
         showlegend = T)


```


From the Figure, we conclude that Zipcodes 10011,10023,1004 have the most revenue by quarter 1 against the total cost

We will now use Percent Units as a key metric.This will tell us that which zipcode has maximum properties.

```{r}
dff <- plot_my_graph(Percent_units) # call the ordering and filtering function using Percent units as key metric

dff$zipcode <- factor(dff$zipcode, levels = unique(dff$zipcode)[order(dff$Percent_units, decreasing = TRUE)])
dff %>%  plot_ly(x = ~zipcode, y = ~Percent_units, type = 'scatter',mode = 'markers', size = ~Percent_units, marker = list(color = c('green','blue','red','grey','grey','grey','grey','grey','grey','grey'))) %>%
  layout(title = "Top zipcodes with maximum percent of properties",
         xaxis = list(title = "Zipcodes"),
         yaxis = list(title = "Percent"))


```



From the above two plots, we can say that the zipcodes with maximum percent of properties are not the ones which generate the maximum revenue by quarter1 against total cost. Meaning, revenue vs cost for a zipcode and total number of properties in a zipcode are not closely correlated . To take this further, we use the below logic.

# Revenue by Q1 vs Zipcodes

```{r}

##### same function with different value of n=2000. This will ask the ordering function(plot_my_graph2) to check and order the top 2000 records , which will cover information of almost every zipcode given information of revenue in q1.

plot_my_graph2 <- function(col_name){
  # sort the dataframe for by col_name in descending order and subset for top N zipcodes
  
  v <- enquo(col_name)
  
  n = 2000  
  df_sorted_unique_id <- arrange(summary_df[summary_df$id>10,],desc(!!v)) [1:n,] # order data
  
  # reassign factor levels
  
  df_sorted_unique_id$zipcode <- factor(df_sorted_unique_id$zipcode)
  
  
  # Return zipcodes
  
  return(df_sorted_unique_id)
  
}


plot_my_graph2(Revenue_by_q1) %>%  plot_ly( x = ~zipcode, y = ~Revenue_by_q1, type = 'bar')%>%
  layout(title = "Top zipcodes by Revenue in Quarter 1",
         xaxis = list(title = "Zipcode"),
         yaxis = list(title = "Revenue"))



```

This message shows that even if n = 1238, then too all the zipcodes would have been queried against revenue.

From the plot above, we can see that the top 4 profitable zipcodes as per revenue by q1 are 10011, 10023,10014 and 10013 . An important stat to notice is that 10011,10023 and 10014 are the most profitable zipcodes as per the first plot (revenue vs cost) . So, if the investors are interested in buying properties which cost less but have higher returns then 10011,10014 and 10023 are the best choice.Even 10013 ranks 5th in terms of revenue in q1 but it comes with a little higher cost.Revenue by Year vs Zipcode as a metric also leads to similar finding.

To be very clear on our findings, lets take another metric , Revenue by Cost Ratio.

```{r}
plot_my_graph2(Cost_by_Revenue) %>%  plot_ly( x = ~zipcode, y = ~Cost_by_Revenue, type = 'bar', marker = list(color = c('green','grey','grey','grey','grey','grey','grey','grey','grey','grey'))) %>%
  layout(title = "Number of Years to break even",
         xaxis = list(title = "Zipcodes"),
         yaxis = list(title = "Cost by Revenue Ratio"))

```

# Conclusion

Zipcodes 10011,10023,10014,10013,10003 and 10036 have substantial number of properties with high roi. These zipcodes also have substantial number of properties listed.

# Executive Summary

The main agenda of the analysis was to find out top zipcodes for a real estate company that wants to buy properties in New York city and put them on rent to have high short term gains. AirBnb and ZillowData were the datasets available for analysis. In simple words, we forecasted the property cost data for particular zipcodes in Zillow Data and compared them with the revenue data (which we generated using R-modeling). This comparison gives us insights on potential profitability , given that the forecasted values are close to the actual price. 

We finally concluded that investment worthy zipcodes should not only have more number of properties but also a good return on investment.**From graphs, it was inferred that zipcodes 10011,10023,10014,1003 and 10036 is agood mix of zipcodes which includes high number of properties and high return on investment too.











